%
% set up May 2013
%
\chapter{Optimization Convergence Algorithms}

Suggest we have an energy functional $E = E(p_{1}, p_{2}, \cdots)$, we want to do the minimization 
work that $\dfrac{\partial E}{\partial p_{i}} = 0$, $i = 1, 2, \cdots$ then how can we do it in
iterative way? We note, that here the parameter $p_{i}$ could be referring to multiple
cases. For example, if the parameter is related to the trial wave function then we are trying to
get a wave function solution. If the parameter is the atomic coordinates then we want to get 
a stationary point in the potential energy surface.

\section{DIIS Method}
%
%
%
\subsection{General Idea about DIIS Method}
%
%
%
DIIS method(direct inversion in the iterative subspace or direct inversion of the iterative subspace)
\cite{Pulay1980393, JCC:JCC540030413} starts from the Newton-Raphson method. This method is used to
speed up the optimization procedure used in SCF process and geometry optimization process in quantum
chemistry. In these practical applications, it's usually hard to get the exact Hessian in the 
equation \ref{Newton-Raphson_eq:9}. Therefore, the question we have here is that how can we use an 
approximated Hessian matrix to perform Newton-Raphson optimization procedure?

Suggest that we use an approximated Hessian matrix $H_{0}$, according to equation
\ref{Newton-Raphson_eq:9}, the approximation solution based on a guess vector $\mathbf{p_{n-1}}$ 
could be expressed as:
\begin{equation}
\label{DIIS_eq:1}
 \mathbf{p_{n}} = \mathbf{p_{n-1}} - H_{0}^{-1}G
\end{equation}
Here $G$ is the gradient vector for $\dfrac{\partial E}{\partial \mathbf{p_{n-1}}}$ and 
$H_{0}$ characterizes its second derivatives matrix.

Suggest that if we have the accurate Hessian matrix, we could express the $G$ as:
\begin{equation}
\label{DIIS_eq:2}
 G = H(\mathbf{p_{n-1}} - \mathbf{p}^{final})
\end{equation}
Where $\mathbf{p}^{final}$ corresponds to the exact solution for equation, which is 
$\dfrac{\partial E}{\partial p_{i}} = 0$, $i = 1, 2, \cdots$. 
Therefore, let's bring the \ref{DIIS_eq:2} into \ref{DIIS_eq:1} we can have:
\begin{equation}
 \label{DIIS_eq:3}
 \mathbf{p_{n}} = \mathbf{p}^{final} + (1-H_{0}^{-1}H)(\mathbf{p_{n-1}} - \mathbf{p}^{final})
\end{equation}

Now let's try to modify the \ref{DIIS_eq:3}:
\begin{align}
 \label{DIIS_eq:4}
 \mathbf{p_{n}} - \mathbf{p_{n-1}} &= \mathbf{p}^{final} - \mathbf{p_{n-1}} + 
 (1-H_{0}^{-1}H)(\mathbf{p_{n-1}} - \mathbf{p}^{final}) \nonumber \\
 &= (\mathbf{p}^{final} - \mathbf{p_{n-1}})(1-(1-H_{0}^{-1}H)) \nonumber \\
 &= (\mathbf{p}^{final} - \mathbf{p_{n-1}})H_{0}^{-1}H
\end{align}
The expression of \ref{DIIS_eq:4} has an important meaning. It indicates that
if the RHS of \ref{DIIS_eq:4} approaches to zero, since
the term of $H_{0}^{-1}H$ is never zero, then the $\mathbf{p}^{final} - \mathbf{p_{n-1}}$ 
is zero - which means, we get to the exact solution of the whole equation! 
Therefore, the difference between $\mathbf{p_{n}} - \mathbf{p_{n-1}}$ characterizes the 
measure for the convergence of the solution, if it's converged; then we get the final
real solution. We name the difference between
$\mathbf{p_{n}}$ and $\mathbf{p_{n-1}}$ as residual vector: $\triangle \mathbf{p_{n}}$, which
should be the norm between the two vectors.

Based on this point, we can suggest an expression that the solution vectors
of $\mathbf{p}$ is expressed as a linear combination of its previous iterative 
vectors:
\begin{equation}
 \label{DIIS_eq:5}
 \mathbf{p_{n}} = \sum_{i}c_{i}\mathbf{p_{i}}
\end{equation}
Therefore, the question here is to how can we generate the coefficients of 
$c_{i}$ so that the $\mathbf{p_{n}}$ could converge to final solution.

First of all, we can see that the $c_{i}$ is actually required to meet some
restrictions. That is 
\begin{equation}
 \label{DIIS_eq:6}
 \sum_{i}c_{i} = 1
\end{equation}
Why we have that? Suggest the final solution is $\mathbf{p}^{final}$,
and we can express each of approximated solution $\mathbf{p_{i}}$ as:
\begin{equation}
 \mathbf{p_{i}} = \mathbf{p}^{final} + \mathbf{e}_{i}
\end{equation}
where $e_{i}$ is the error estimation.Then we have:
\begin{align}
\label{DIIS_eq:7}
 \mathbf{p_{n}} &= \sum_{i}c_{i}(\mathbf{p}^{final} + \mathbf{e}_{i}) \nonumber \\
 &= \mathbf{p}^{final}\sum_{i}c_{i} + \sum_{i}c_{i}\mathbf{e}_{i}
\end{align}
If we have the requirement shown in \ref{DIIS_eq:6}, then as the errors of $\mathbf{e}_{i}$
goes to zero, then in the above equation we have $\mathbf{p_{n}} = \mathbf{p}^{final}$.

Next, how can we evaluate these $c_{i}$ in \ref{DIIS_eq:5}? According to the result
shown in \ref{DIIS_eq:4}, the residual vector of $\triangle \mathbf{p_{n}}$ characterizes
the convergence of approximated solutions towards the exact solution, therefore it's 
natral to imagine that if all of the $\triangle \mathbf{p_{i}}$ ($i=1,2,\cdots$) are small
enough, then the result $c_{i}$ must be the best approximation. Such idea is similar to the 
least square problem. Therefore we have:
\begin{equation}
\label{DIIS_eq:8}
 \triangle \mathbf{P} = \sum_{i}c_{i}\triangle \mathbf{p_{i}} 
 \Rightarrow \min(\triangle \mathbf{P})
\end{equation}

For evaluating the minimum of $\triangle \mathbf{P}$, we can construct some Lagrangian:
\begin{align}
 \label{DIIS_eq:9}
 L &= \langle \triangle \mathbf{P} |\triangle \mathbf{P} \rangle - 
 \lambda(1-\sum_{i}c_{i}) \nonumber \\
   &= \sum_{i}\sum_{j}c_{i}c_{j}\langle\triangle \mathbf{p_{i}}|\triangle\mathbf{p_{j}}\rangle - 
   \lambda(1-\sum_{i}c_{i})
\end{align}

By requiring that $\dfrac{\partial L}{\partial c_{i}} = 0$ ($i = 1, 2, \cdots$) 
as well as $\dfrac{\partial L}{\partial \lambda} = 0$ we can have:
\begin{align}
 \frac{\partial L}{\partial c_{i}} &= 
 2\sum_{j}c_{j}\langle\triangle \mathbf{p_{i}}|\triangle\mathbf{p_{j}}\rangle - \lambda
 = \sum_{j}c_{j}\langle\triangle \mathbf{p_{i}}|\triangle\mathbf{p_{j}}\rangle - \frac{\lambda}{2}
 \nonumber \\
 \frac{\partial L}{\partial \lambda} &= 
 1-\sum_{i}c_{i}
\end{align}
We note that the factor of 2 is absorbed into the $\lambda$ in above equation.

By applying the above derivation into the \ref{DIIS_eq:9}, then we can have the following 
linear equation:
\begin{align}
 \label{DIIS_eq:10}
 &\begin{bmatrix}
  \langle\triangle \mathbf{p_{1}}|\triangle\mathbf{p_{1}}\rangle  &
  \langle\triangle \mathbf{p_{1}}|\triangle\mathbf{p_{2}}\rangle  &
  \cdots                                                          &
  \langle\triangle \mathbf{p_{1}}|\triangle\mathbf{p_{m}}\rangle  &
   -1                                                             \\
  \langle\triangle \mathbf{p_{2}}|\triangle\mathbf{p_{1}}\rangle  &
  \langle\triangle \mathbf{p_{2}}|\triangle\mathbf{p_{2}}\rangle  &
  \cdots                                                          &
  \langle\triangle \mathbf{p_{2}}|\triangle\mathbf{p_{m}}\rangle  &
   -1                                                             \\
  \cdots                                                          &
  \cdots                                                          &
  \cdots                                                          &
  \cdots                                                          &
  \cdots                                                          \\
  \langle\triangle \mathbf{p_{m}}|\triangle\mathbf{p_{1}}\rangle  &
  \langle\triangle \mathbf{p_{m}}|\triangle\mathbf{p_{2}}\rangle  &
  \cdots                                                          &
  \langle\triangle \mathbf{p_{m}}|\triangle\mathbf{p_{m}}\rangle  &
  -1                                                             \\  
  -1                                                              &
  -1                                                              &
  \cdots                                                          &
  -1                                                              &
   0                                                             \\
 \end{bmatrix}
 \begin{bmatrix}
  c_{1}  \\
  c_{2}  \\
  \vdots \\
  c_{m}  \\
  \lambda\\ 
 \end{bmatrix}
&=  \begin{bmatrix}
   0  \\
   0  \\
  \vdots \\
   0  \\
   1  \\ 
 \end{bmatrix}
\end{align}
The final DIIS step is to obtain a solution to the above linear equation, then 
we know how to form the new vector of $\mathbf{p}$.

Physically, in DIIS procedure the most important expression is the \ref{DIIS_eq:8}.
This expression explains how we can get the coefficients of $c_{i}$. Furthermore,
we note that this expression is not unique. We can have multiple way to create the 
residual vectors of $\triangle \mathbf{p}$ so that to have better convergence for the 
optimization process.

Another thing to note is that the matrix in \ref{DIIS_eq:10} could be singular when
the optimization process is near to the convergence. For example, if the last two
error vectors of $\triangle \mathbf{p_{m}}$ and $\triangle \mathbf{p_{m-1}}$ are 
nearly same, then their corresponding column and rows would be nearly same - causing
the matrix to be singular.

At last, it is interesting to note that DIIS procedure has been mathematically analyzed.
It's shown that DIIS procedure is equivalent to the quasi-Newton/Secant method
\cite{rohwedder2011analysis}.

\subsection{DIIS in SCF Procedure}
%
%
The key in the DIIS procedure, is to find a proper expression of the error vectors 
defined in \ref{DIIS_eq:8}. In paper ~\cite{JCC:JCC540030413} Pulay suggest to use 
the difference below:
\begin{equation}
 \label{DIIS_SCF_eq:1}
 \triangle \mathbf{p} = FPS - SPF
\end{equation}
to represent the errors in SCF convergence. We note, that it coincides our expression 
in \ref{HF_density_matrix:eq:5} that the converged density matrix is able to commute with
Fock matrix in MO manner.

How to perform the DIIS procedure in SCF cycles? Generally it could be divided into the 
following steps:
\begin{enumerate}
 \item Building density matrices $P$ and overlap matrix $S$;
 \item Building Fock matrices according to the number of spin states;
 \item Construct error vectors $e$ for each spin state according to \ref{DIIS_SCF_eq:1};
 \item Transform the error vectors into form of $S^{-\frac{1}{2}}eS^{-\frac{1}{2}}$ so that
 to make it based on orthogonal basis sets\footnote{another way is that we can do such 
 transformation to the Fock matrix itself after the Fock matrix was built};
 \item Estimate the maximum error and root mean square error and compare it with the criteria 
 to see whether the convergence has been achieved;
 \item Extrapolate the error matrix defined in \ref{DIIS_eq:10}, we note that it's only the 
 last column/row we need to re-compute, and the new elements are set to be $ E_{ij} = 
 \langle e_{i}|e_{j}\rangle$;
 \item Solve the error matrix and get the coefficients of $c_{i}$;
 \item Construct the new Fock matrix based on the expression of \ref{DIIS_eq:5}, where the 
 vector of $\mathbf{p}$ is actually the previous Fock matrix.
\end{enumerate}










